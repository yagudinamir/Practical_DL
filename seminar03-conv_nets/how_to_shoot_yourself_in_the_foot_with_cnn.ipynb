{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "how_to_shoot_yourself_in_the_foot_with_cnn.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9knWAIelap5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch, torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# a special module that converts [batch, channel, w, h] to [batch, units]\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxAOHFvglszx",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yandexdataschool/Practical_DL/blob/spring20/seminar3/how_to_shoot_yourself_in_the_foot_with_cnn.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W6k6Bullap_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# assuming input shape [batch, 3, 64, 64]\n",
        "cnn = nn.Sequential(\n",
        "    nn.Conv2d(in_channels=3, out_channels=2048, kernel_size=(3,3)),\n",
        "    nn.Conv2d(in_channels=2048, out_channels=1024, kernel_size=(3,3)),\n",
        "    nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=(3,3)),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d((6,6)),\n",
        "    nn.Conv2d(in_channels=6, out_channels=32, kernel_size=(20,20)),\n",
        "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(20,20)),\n",
        "    nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(20,20)),\n",
        "    nn.Softmax(),\n",
        "    Flatten(),\n",
        "    nn.Linear(64, 256),\n",
        "    nn.Softmax(),\n",
        "    nn.Linear(256, 10),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Dropout(0.5)\n",
        "    \n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xydlW1NSlaqD",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "# Book of grudges\n",
        "* Input channels are wrong literally half the time (after pooling, after flatten).\n",
        "* Too many filters for first 3x3 convolution - will lead to enormous matrix while there's just not enough relevant combinations of 3x3 images (overkill).\n",
        "* Usually the further you go, the more filters you need.\n",
        "* large filters (10x10 is generally a bad pactice, and you definitely need more than 10 of them\n",
        "* the second of 10x10 convolution gets 8x6x6 image as input, so it's technically unable to perform such convolution.\n",
        "* Softmax nonlinearity effectively makes only 1 or a few neurons from the entire layer to \"fire\", rendering 512-neuron layer almost useless. Softmax at the output layer is okay though\n",
        "* Dropout after probability prediciton is just lame. A few random classes get probability of 0, so your probabilities no longer sum to 1 and crossentropy goes -inf."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E84fiu7ZlaqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}