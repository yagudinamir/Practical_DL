{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "style_transfer_pytorch.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsKCO-s_VdCf",
        "colab_type": "text"
      },
      "source": [
        "### Neural style transfer in PyTorch\n",
        "\n",
        "This tutorial implements the \"slow\" neural style transfer based on the VGG19 model.\n",
        "\n",
        "It closely follows the official neural style tutorial you can find [here](http://pytorch.org/tutorials/advanced/neural_style_tutorial.html).\n",
        "\n",
        "__Note:__ if you didn't sit through the explanation of neural style transfer in the on-campus lecture, you're _strongly recommended_ to follow the link above instead of this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RanDMavdVdDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from matplotlib.pyplot import imread\n",
        "from skimage.transform import resize\n",
        "\n",
        "import torch, torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# desired size of the output image\n",
        "imsize = 256   # REDUCE THIS TO 128 IF THE OPTIMIZATION IS TOO SLOW FOR YOU\n",
        "def image_loader(image_name):\n",
        "    image = resize(imread(image_name), [imsize, imsize])\n",
        "    image = image.transpose([2,0,1]) / image.max()\n",
        "    image = Variable(dtype(image))\n",
        "    # fake batch dimension required to fit network's input dimensions\n",
        "    image = image.unsqueeze(0)\n",
        "    return image\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "print(\"torch\", torch.__version__)\n",
        "if use_cuda:\n",
        "    print(\"Using GPU.\")\n",
        "else:\n",
        "    print(\"Not using GPU.\")\n",
        "dtype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DpdYNl_VdEA",
        "colab_type": "text"
      },
      "source": [
        "### Draw input images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD5VMJACVdEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm Practical_DL* -rf && wget https://github.com/yandexdataschool/Practical_DL/archive/spring20.zip && unzip spring20.zip && mv */seminar06-style-transfer/images .\n",
        "style_img = image_loader(\"images/wave.jpg\").type(dtype)\n",
        "\n",
        "!wget http://cdn.cnn.com/cnnnext/dam/assets/170809210024-trump-nk.jpg -O images/my_img.jpg\n",
        "content_img = image_loader(\"images/my_img.jpg\").type(dtype)\n",
        "\n",
        "assert style_img.size() == content_img.size(), \\\n",
        "    \"we need to import style and content images of the same size\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2vUr26oVdEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(tensor, title=None):\n",
        "    image = tensor.clone().cpu()  # we clone the tensor to not do changes on it\n",
        "    image = image.view(3, imsize, imsize)  # remove the fake batch dimension\n",
        "    image = image.numpy().transpose([1,2,0])\n",
        "    plt.imshow(image / np.max(image))\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "\n",
        "plt.figure(figsize=[12,6])\n",
        "plt.subplot(1,2,1)\n",
        "imshow(style_img.data, title='Style Image')\n",
        "plt.subplot(1,2,2)\n",
        "imshow(content_img.data, title='Content Image')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9eUhLrpVdEo",
        "colab_type": "text"
      },
      "source": [
        "### Define Style Transfer Losses\n",
        "\n",
        "As shown in the lecture, we define two loss functions: content and style losses.\n",
        "\n",
        "Content loss is simply a pointwise mean squared error of high-level features while style loss is the error between gram matrices of intermediate feature layers.\n",
        "\n",
        "To obtain the feature representations we use a pre-trained VGG19 network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_SOvPRZVdEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "cnn = models.vgg19(pretrained=True).features\n",
        "\n",
        "# move it to the GPU if possible:\n",
        "if use_cuda:\n",
        "    cnn = cnn.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ityNinPtVdE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ContentLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, target, weight):\n",
        "        super(ContentLoss, self).__init__()\n",
        "        # we 'detach' the target content from the tree used\n",
        "        self.target = target.detach() * weight\n",
        "        self.weight = weight\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.loss = F.mse_loss(input * self.weight, self.target)\n",
        "        return input.clone()\n",
        "\n",
        "    def backward(self, retain_graph=True):\n",
        "        self.loss.backward(retain_graph=retain_graph)\n",
        "        return self.loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8ilRY_8VdFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gram_matrix(input):\n",
        "    a, b, c, d = input.size()  # a=batch size(=1)\n",
        "    # b=number of feature maps\n",
        "    # (c,d)=dimensions of a f. map (N=c*d)\n",
        "\n",
        "    features = input.view(a * b, c * d)  # resise F_XL into \\hat F_XL\n",
        "\n",
        "    G = torch.mm(features, features.t())  # compute the gram product\n",
        "\n",
        "    # we 'normalize' the values of the gram matrix\n",
        "    # by dividing by the number of element in each feature maps.\n",
        "    return G.div(a * b * c * d)\n",
        "    \n",
        "class StyleLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, target, weight):\n",
        "        super(StyleLoss, self).__init__()\n",
        "        self.target = target.detach() * weight\n",
        "        self.weight = weight\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.G = gram_matrix(input)\n",
        "        self.G.mul_(self.weight)\n",
        "        self.loss = F.mse_loss(self.G, self.target)\n",
        "        return input.clone()\n",
        "\n",
        "    def backward(self, retain_graph=True):\n",
        "        self.loss.backward(retain_graph=retain_graph)\n",
        "        return self.loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSmXzF0pVdFR",
        "colab_type": "text"
      },
      "source": [
        "### Style transfer pipeline\n",
        "\n",
        "We can now define a unified \"model\" that computes all the losses on the image triplet (content image, style image, optimized image) so that we could optimize them with backprop (over image pixels)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OV8FzGTrVdFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "content_weight = 1            # coefficient for content loss\n",
        "style_weight = 1000           # coefficient for style loss\n",
        "content_layers = ('conv_4',)  # use these layers for content loss\n",
        "style_layers = ('conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5') # use these layers for style loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWMabOdBVdFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "content_losses = []\n",
        "style_losses = []\n",
        "\n",
        "model = nn.Sequential()  # the new Sequential module network\n",
        "# move these modules to the GPU if possible:\n",
        "if use_cuda:\n",
        "    model = model.cuda()\n",
        "\n",
        "i = 1\n",
        "for layer in list(cnn):\n",
        "    if isinstance(layer, nn.Conv2d):\n",
        "        name = \"conv_\" + str(i)\n",
        "        model.add_module(name, layer)\n",
        "\n",
        "        if name in content_layers:\n",
        "            # add content loss:\n",
        "            target = model(content_img).clone()\n",
        "            content_loss = ContentLoss(target, content_weight)\n",
        "            model.add_module(\"content_loss_\" + str(i), content_loss)\n",
        "            content_losses.append(content_loss)\n",
        "\n",
        "        if name in style_layers:\n",
        "            # add style loss:\n",
        "            target_feature = model(style_img).clone()\n",
        "            target_feature_gram = gram_matrix(target_feature)\n",
        "            style_loss = StyleLoss(target_feature_gram, style_weight)\n",
        "            model.add_module(\"style_loss_\" + str(i), style_loss)\n",
        "            style_losses.append(style_loss)\n",
        "\n",
        "    if isinstance(layer, nn.ReLU):\n",
        "        name = \"relu_\" + str(i)\n",
        "        model.add_module(name, layer)\n",
        "\n",
        "        if name in content_layers:\n",
        "            # add content loss:\n",
        "            target = model(content_img).clone()\n",
        "            content_loss = ContentLoss(target, content_weight)\n",
        "            model.add_module(\"content_loss_\" + str(i), content_loss)\n",
        "            content_losses.append(content_loss)\n",
        "\n",
        "        if name in style_layers:\n",
        "            # add style loss:\n",
        "            target_feature = model(style_img).clone()\n",
        "            target_feature_gram = gram_matrix(target_feature)\n",
        "            style_loss = StyleLoss(target_feature_gram, style_weight)\n",
        "            model.add_module(\"style_loss_\" + str(i), style_loss)\n",
        "            style_losses.append(style_loss)\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    if isinstance(layer, nn.MaxPool2d):\n",
        "        name = \"pool_\" + str(i)\n",
        "        model.add_module(name, layer)  # ***"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DU8nQ8AXVdF4",
        "colab_type": "text"
      },
      "source": [
        "### Optimization\n",
        "\n",
        "We can now optimize both style and content loss over input image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flkw9uf_VdF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_image = Variable(content_img.clone().data, requires_grad=True)\n",
        "optimizer = torch.optim.LBFGS([input_image])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "x4h1qaZQVdGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_steps = 300\n",
        "\n",
        "for i in range(num_steps):\n",
        "    # correct the values of updated input image\n",
        "    input_image.data.clamp_(0, 1)\n",
        "\n",
        "    model(input_image)\n",
        "    style_score = 0\n",
        "    content_score = 0\n",
        "    for sl in style_losses:\n",
        "        style_score += sl.backward()\n",
        "    for cl in content_losses:\n",
        "        content_score += cl.backward()\n",
        "        \n",
        "    if i % 10 == 0:  # <--- adjust the value to see updates more frequently\n",
        "        \n",
        "        print('Step # {} Style Loss : {:4f} Content Loss: {:4f}'.format(\n",
        "            i, style_score.item(), content_score.item()))\n",
        "        plt.figure(figsize=[10,10])\n",
        "        imshow(input_image.data)\n",
        "        plt.show()\n",
        "        \n",
        "    loss = style_score + content_score\n",
        "    \n",
        "    optimizer.step(lambda:loss)\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "# a last correction...\n",
        "input_image.data.clamp_(0, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLuj8OBJVdGR",
        "colab_type": "text"
      },
      "source": [
        "### Final image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqC_yYeWVdGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=[10,10])\n",
        "plt.imshow(input_image.cpu().data.numpy()[0].transpose(1, 2, 0))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}